# -*- coding: utf-8 -*-
"""feature_vector2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ziURfvwi2o6hRyMC8zRezAyr3a2jfMPx
"""

import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import cv2
import torch
import skimage.transform
import numpy as np
from PIL import Image
#from resizeimage import resizeimage
import pandas as pd

def cropping(imagename):
    y=[]
    im = cv2.imread(imagename)
    h, w, _ = im.shape
    bb = pd.read_csv(imagename.split('.')[0] + '.txt', header = None)
    l=[]
    for i in range(len(bb)):
        for j in range(4):
            l.append(bb[0][i].split(' ')[j+1])
    for i in range(0, 4*len(bb), 4):
        voc = []
        data = [l[i], l[i+1], l[i+2], l[i+3]]
        bbox_width = float(data[2]) * w
        bbox_height = float(data[3]) * h
        center_x = float(data[0]) * w
        center_y = float(data[1]) * h
        voc.append(int(center_x - (bbox_width / 2)))
        voc.append(int(center_y - (bbox_height / 2)))
        voc.append(int(center_x + (bbox_width / 2)))
        voc.append(int(center_y + (bbox_height / 2)))

        return_im = im[voc[1]:voc[3], voc[0]:voc[2]]
        
        serial = (i/4) + 1

        cv2.imwrite(imagename.split('.')[0] + str(serial) +'.jpg',return_im)
        y.append(imagename.split('.')[0] + str(serial) +'.jpg')
    
    return y

(cropping('biking.jpg'))

class feature_extract(nn.Module):
    
    def __init__(self, embed_size):
        super(feature_extract, self).__init__()
        resnet = models.resnet152(pretrained = True)
        modules = list(resnet.children())[:-1]
        self.resnet = nn.Sequential(*modules)
        self.linear1 = nn.Linear(resnet.fc.in_features, embed_size)
        self.linear2 = nn.Linear(resnet.fc.in_features, embed_size)
        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)
        self.dropout = nn.Dropout2d(p=0.5, inplace=False)
        #self.image = image
        
    def forward(self, images):
        images = images.float()
        with torch.no_grad():
            features = self.resnet(images)
        #print(features.size())
        features = features.view(-1, 2048)
        #features = features.reshape(features.size(0), -1)
        features = self.linear1(features)
        features = self.dropout(self.linear2(features))
        return features

def get_image(imagename, hi, wi):
        img = cv2.imread(imagename)
        #print(img.shape)
        image_file = skimage.transform.resize(image=img, output_shape=(1, 3, hi, wi))
        #image_file = resizeimage.resize_cover(img, [1, 3, hi, wi])
        #image_file = img.resize((1, 3, hi, wi),  Image.BICUBIC)
        #image_file = cv2.resize(img, (hi, wi))
        #image_file = image_file.T
        
        #image_file = np.expand_dims(image_file, 0)
        #print(image_file.shape)
        img_arr = np.asarray(image_file)
        return img_arr

def get_features(imagename, hi, wi, embed_size):
    img_arr = get_image(imagename, hi, wi)
    b = torch.from_numpy(img_arr)
    f = feature_extract(embed_size)
    f1 = f(b)
    return f1

g1 = get_features('biking1.0.jpg', 224, 224, 2048)

g2 = get_features('biking2.0.jpg', 224, 224, 2048)

g_m = torch.cat((g1, g2), 0)

g_m.size()

def feature_adding(imagename, hi, wi, embed_size):
    u = []
    #h = []
    for i in range(len(cropping(imagename))):
        u.append(get_features(cropping(imagename)[i], 224, 224, 2048))
        final = torch.cat(u, 0)
    print(final.size())

feature_adding('biking.jpg', 224, 224, 2048)

class DecoderRNN(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):
        """Set the hyper-parameters and build the layers."""
        super(DecoderRNN, self).__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.linear = nn.Linear(hidden_size, vocab_size)
        self.max_seg_length = max_seq_length
        
    def forward(self, features, captions, lengths):
        """Decode image feature vectors and generates captions."""
        embeddings = self.embed(captions)
        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)
        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) 
        hiddens, _ = self.lstm(packed)
        outputs = self.linear(hiddens[0])
        return outputs
    
    def sample(self, features, states=None):
        """Generate captions for given image features using greedy search."""
        sampled_ids = []
        inputs = features.unsqueeze(1)
        for i in range(self.max_seg_length):
            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)
            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)
            _, predicted = outputs.max(1)                        # predicted: (batch_size)
            sampled_ids.append(predicted)
            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)
            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)
        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)
        return sampled_ids

